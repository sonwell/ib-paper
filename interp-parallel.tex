\subsection{Parallelization of interpolation}

From the above, we can see that $\mathcal{S}$ has approximately equal number
of nonzero entries per column. This means, that the interpolation matrix,
$\mathcal{S}^\dagger$, has approximately equal number of nonzero entries per row.
This property is beneficial for parallelization. Consider the $j^\text{th}$ row of
$\mathcal{S}^\dagger$, which corresponds to interpolating to Lagrangian point
$\vec{X}_j$ using the values at its support points. There are at most $s^d$
values in that row, which correspond to the shifts that give a potentially
nonzero value for $\delta_h$. Compute $\vec{x} = h(\lfloor\vec{X}_j\rceil+\vec{g})$.
Then $\Delta\vec{X} = \vec{X}_j-\vec{x}$. Now, since the shifts $\{\vec{\sigma}_i\}$
are known beforehand, we can compute $\delta_h(\vec{\sigma}_i-\Delta\vec{X})$
and, if $\#(\vec{\sigma}_i+\lfloor\vec{x}\rceil) \neq\epsilon$, we accumulate
products
\begin{equation*}
    E_j = \sum_{i=1}^{s^d}\delta_h(\vec{\sigma}_i+\Delta\vec{X})e_{\#(\vec{\sigma}_i+\lfloor\vec{x}\rceil)}.
\end{equation*}

Assigning one thread per Lagrangian point (i.e., one thread per row), this
calculation can be performed in parallel, and since the $j^\text{th}$ thread writes
to the $j^\text{th}$ entry of $\vec{E}$, there are no write contentions. Because the
number of products is approximately the same for each row, each thread does
approximately the same amount of work. On architectures that enforce thread
synchrony, such as GPUs, this means that we do not incur a penalty from having
threads wait for other threads to finish.

Since each thread computes the appropriate $\delta_h$-weights for its own row,
it is unnecessary to construct $\mathcal{S}^\dagger$ explicitly. Other than
allocating memory for $\vec{E}$, all of the work for this algorithm is parallel,
so we expect to see near-perfect scaling and a theoretical runtime of
$\mathcal{O}(n_\gamma/p)$, where $p$ is the number of threads.
