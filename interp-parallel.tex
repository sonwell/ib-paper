\subsection{Parallelization of interaction operators}

Below, we describe a method for evaluating
\begin{align}
    \label{eq:scalar-interp}
    E(\vec{X}) &= \int_\Omega \delta_h(\vec{x}-\vec{X})e(\vec{x}) \d\omega \quad \text{and}\\
    \label{eq:scalar-spread}
    \ell(\vec{x}) &= \int_\Gamma \delta_h(\vec{x}-\vec{X})L(\vec{X}) \d\gamma
\end{align}
for scalar-valued functions $e: \Omega\to\mathbb{R}$ and $L:\Gamma\to\mathbb{R}$.
For vector-valued functions, such as $\vec{u}$ and $\vec{F}$, the algorithm
can be applied to each component individually. For some grids, it may be
possible to process each component concurrently. We consider this possibility
at the end of the section.

\subsection{The structure of $\mathcal{S}$ and $\mathcal{S}^\dagger$}

Let $\Omega_h$ be a regular grid on $\Omega$ with grid spacing $h$. Define
$\vec{g} \in [0,\,1)^d$ to be the fixed vector such that a grid point $\vec{x}$
can be decomposed as $\vec{x}=h(\vec{i}+\vec{g})$, where $\vec{i}$ has integer
components. Moreover, let $\Gamma_h$ be a discretization of the interface
$\Gamma$. For illustration purposes, we can think of $\Gamma_h$ as a collection
of arbitrary points in $\Omega$. We refer to points in $\Omega_h$ as (Eulerian)
grid ponts, and points in $\Gamma_h$ as Lagrangian points.

[Equations @eq:scalar-interp;@eq:scalar-spread] are discretized on $\Omega_h$
and $\Gamma_h$, respectively, to yield
\begin{align}
    \label{eq:disc-interp}
    E(\vec{X}_j) &= \sum_i \delta_h(\vec{x}_i-\vec{X}_j)e(\vec{x}_i) h^d \quad \text{and} \\
    \label{eq:disc-spread}
    \ell(\vec{x}_i) &= \sum_j \delta_h(\vec{x}_i-\vec{X}_j)L(\vec{X}_j) \d A.
\end{align}
Each of the above equations look like a matrix-vector multiplication, so we 
will define $\mathcal{S}=(\delta_h(\vec{x}_i-\vec{X}_j))$, where the subscript
$i$ indicates the row and subscript $j$ indicates the column. Collecting the
values of $e(\vec{x})$ at each Eulerian grid point and of $L(\vec{X})$ at
each Lagrangian point, we rewrite [equations @eq:disc-interp;@eq:disc-spread]
as
\begin{align}
    \label{eq:matrix-interp}
    \vec{E} &= \mathcal{S}^\dagger\vec{e} \quad\text{and} \\
    \label{eq:matrix-spread}
    \vec{\ell} &= \mathcal{S}\vec{L},
\end{align}
respectively. For this reason, we call $\mathcal{S}$ the \emph{spread matrix},
and its transpose, $\mathcal{S}^\dagger$, the \emph{interpolation matrix}.

As we have said, $\delta_h$ is the tensor product of scaled, one-dimensional
kernels, $h^{-1}\phi(h^{-1}x)$. Let $\mathrm{supp}\mskip\thinmuskip\phi$
denote the support of $\phi$ and define
\begin{equation}
    s[\phi] = |\mathrm{supp}\mskip\thinmuskip\phi\cap\mathbb{Z}|-1
\end{equation}
to be the size of the support in meshwidths. For brevity, we write $s=s[\phi]$.
For any $\vec{X}\in\Omega$, there are at most $s^d$ grid points
$\vec{x}\in\Omega_h$ for which $\delta_h(\vec{x}-\vec{X})$ is nonzero.

Let $\vec{y}\in\Omega$ be an arbitrary point and consider the set of grid
points for which $\delta_h(\vec{x}-\vec{y})$ and $\vec{x}\in\Omega_h$. Denote
this set of grid points $\Sigma(\vec{y})$, called the \emph{support points}
of $\vec{y}$. The pre-image $\Sigma^{-1}(\Sigma(\vec{y}))$ is a subset of
$\Omega$ containing at most one grid point. For $\vec{y}$ sufficiently far away
from any boundary, $\Sigma^{-1}(\Sigma(\vec{y}))$ is an $h \times h$ ($d=2$) or
$h \times h \times h$ ($d=3$) subset of $\Omega$. For points near a boundary,
the region may be smaller. Collectively, these regions cover $\Omega$, so we
consider them to be the \emph{de facto} grid cells. For those grid cells that
do not contain a grid point, we extend $\Omega_h$, in a regular way, with ghost
points. Call this extension $\bar{\Omega}_h$. Now, any $h \times h$ or
$h \times h \times h$ extended grid cell that entirely contains a grid cell
also contains exactly one grid point in $\bar{\Omega}_h$, including grid cells
near a boundary. We can identify an $\vec{y}\in\Omega$ with a grid point
$\vec{x}\in\bar{\Omega}_h$ if they are in the same extended grid cell, and
since $\vec{x}=h(\vec{i}=\vec{g})$, we identify a grid cell by the integers
$\vec{i}$. Finally, we define $\lfloor\cdot\rceil:\Omega\to\mathbb{Z}$ such
that $\lfloor\vec{y}\rceil = \vec{i}$.

We now turn our attention to the evaluation of $\delta_h(\vec{x}-\vec{X})$. We
assume $\vec{x}\in\Omega_h$ and write
\begin{equation}
    \label{eq:delta-defs}
    \begin{aligned}
        \delta_h(\vec{x}-\vec{X})
        &= \delta_h(\vec{x}-h(\lfloor\vec{X}\rceil+\vec{g}) + h(\lfloor\vec{X}\rceil+\vec{g}) - \vec{X}) \\
        &= \delta_h(h\vec{\sigma} - \Delta\vec{X}) \\
        &= \prod_{i=1}^d h^{-1}\phi((\vec{\sigma} - h^{-1}\Delta\vec{X})\cdot\vec{c}_i).
    \end{aligned}
\end{equation}
where $\Delta\vec{X}$ is the displacement of $\vec{X}$ from its associated grid
point, $\vec{\sigma} = \lfloor\vec{x}\rceil-\lfloor\vec{X}\rceil$ since
$\vec{x} = h(\lfloor\vec{x}\rceil+\vec{g})$, and $\vec{c}_i$ is the $i^\text{th}$
canonical basis vector. We refer to $\vec{\sigma}$ as a \emph{shift}. Shifts
that result in a possibly nonzero value of $\delta_h$ are known \emph{a priori}
based on $\phi$, and usually range from $-\lfloor s/2\rfloor$ to
$\lfloor(s-1)/2\rfloor$ in each component. We can therefore choose an order for
the shifts $\{\vec{\sigma}_i\}_{i=1}^{s^d}$. We denote the $i^\text{th}$ shift
$\vec{\sigma}_i$.

We need one more ingredient to construct $\mathcal{S}$. Let $\vec{x}_k$ be the
be the $k^\text{th}$ grid point, such that, e.g., $e_k = e(\vec{x}_k)$ is the $k^\text{th}$
entry of $\vec{e}$. The grid point $\vec{x}_k$ can be decomposed into
$h(\vec{i}+\vec{g})$ for some $\vec{i}$ with integer components. Define the
grid indexing function $\#:\mathbb{Z}^d\to\mathbb{Z}\cup\{\epsilon\}$ such that
$\#(\lfloor\vec{x}_k\rceil) = \#(\vec{i}) = k$ for all grid points and
$\#(\vec{i}')=\epsilon$ if $h(\vec{i}'+\vec{g}) \not\in \Omega$.

We are now ready to construct $\mathcal{S}$. Consider a Lagrangian point
$\vec{X}_j$ that is in the same grid cell as grid point
$\vec{x}_k=h(\lfloor\vec{X}_j\rceil+\vec{g})$. The $j^\text{th}$ column of
$\mathcal{S}$ is zero except for up to $s^d$ values where for
$i=1,\,\ldots,\,s^d$, if $\#(\lfloor\vec{X}_j\rceil+\vec{\sigma}_i)\neq\epsilon$,
\begin{equation}
    \label{eq:s-columnwise}
    \mathcal{S}_{i,\#(\lfloor\vec{X}_j\rceil + \vec{\sigma}_i)} = \delta_h(h\vec{\sigma}_i-\vec{X}_j+\vec{x}_k).
\end{equation}

\subsection{Parallelization of interpolation}
From the above, we can see that $\mathcal{S}$ has approximately equal number
of nonzero entries per column. This means, that the interpolation matrix,
$\mathcal{S}^\dagger$, has approximately equal number of nonzero entries per row.
This property is beneficial for parallelization. Consider the $j^\text{th}$ row of
$\mathcal{S}^\dagger$, which corresponds to interpolating to Lagrangian point
$\vec{X}_j$ using the values at its support points. There are at most $s^d$
values in that row, which correspond to the shifts that give a potentially
nonzero value for $\delta_h$. Compute $\vec{x} = h(\lfloor\vec{X}_j\rceil+\vec{g})$.
Then $\Delta\vec{X} = \vec{X}_j-\vec{x}$. Now, since the shifts $\{\vec{\sigma}_i\}$
are known beforehand, we can compute $\delta_h(\vec{\sigma}_i-\Delta\vec{X})$
and, if $\#(\vec{\sigma}_i+\lfloor\vec{x}\rceil) \neq\epsilon$, we accumulate
products
\begin{equation*}
    E_j = \sum_{i=1}^{s^d}\delta_h(\vec{\sigma}_i+\Delta\vec{X})e_{\#(\vec{\sigma}_i+\lfloor\vec{x}\rceil)}.
\end{equation*}

Assigning one thread per Lagrangian point (i.e., one thread per row), this
calculation can be performed in parallel, and since the $j^\text{th}$ thread writes
to the $j^\text{th}$ entry of $\vec{E}$, there are no write contentions. Because the
number of products is approximately the same for each row, each thread does
approximately the same amount of work. On architectures that enforce thread
synchrony, such as GPUs, this means that we do not incur a penalty from having
threads wait for other threads to finish.

Since each thread computes the appropriate $\delta_h$-weights for its own row,
it is unnecessary to construct $\mathcal{S}^\dagger$ explicitly. Other than
allocating memory for $\vec{E}$, all of the work for this algorithm is parallel,
so we expect to see near-perfect scaling and a theoretical runtime of
$\mathcal{O}(n_\gamma/p)$, where $p$ is the number of threads.

\subsection{Parallelization of spread}
The difficulty arises in attempting to parallelize the spread operation.
Row-wise parallelization of $\mathcal{S}\vec{L}$ yields threads doing vastly
different amounts of work -- many no work at all -- and incurring the penalty
of having threads wait for other threads to finish before being able to
continue. Moreover, this parallelization would scale according to the size of
the Eulerian grid rather than the number of Lagrangian points.

Instead, we wish to partition the work so that every thread does a similar
amount of work, it scales well, and depends minimally on the size of the
background grid. We can think of this as finding matrices $\mathcal{S}_1$,
$\mathcal{S}_2$, \dots, $\mathcal{S}_m$ and vectors $\vec{L}_1$, $\vec{L}_2$,
\dots, $\vec{L}_m$ such that
\begin{equation*}
    \vec{\ell} = \mathcal{S}\vec{L} = \mathcal{S}_1\vec{L}_1 + \mathcal{S}_2\vec{L}_2 + \cdots + \mathcal{S}_m\vec{L}_m.
\end{equation*}
Here, $m$ is the number of serial operations or \emph{sweeps} needed to
completely compute $\mathcal{S}\vec{L}$. The products $\mathcal{S}_i\vec{L}_i$
are accumulated serially, but we aim to construct the matrices
$\{\mathcal{S}_i\}$ in a way that allows for efficient parallel computation of
each individual product.

Consider two points in different grid cells, $\vec{x}_1$ and $\vec{x}_2$. Since
these points are in different grid cells, the support points
$h(\lfloor\vec{x}_1\rceil+\vec{g}+\vec{\sigma}_i)$ of $\vec{x}_1$ and 
$h(\lfloor\vec{x}_2\rceil+\vec{g}+\vec{\sigma}_i)$ of $\vec{x}_2$, assuming
they exist, are distinct for $i=1,\,\ldots,\,s^d$. Thus, for any set
$\{\vec{X}_j\}$ of Lagrangian points, each in different grid cells, the values
\begin{equation*}
    \delta_h(h(\lfloor\vec{X}_j\rceil + \vec{g}+\vec{\sigma}_i)-\vec{X}_j)L_j,
\end{equation*}
for $\#(\lfloor\vec{X}_j\rceil+\vec{\sigma}_i)\neq\epsilon$, can be computed
and written in parallel for fixed $i$, because the values of
$\#(\lfloor\vec{X}_j\rceil+\vec{\sigma}_i)$, when not equal to $\epsilon$, are
distinct, and gives the index of the output entry of $\vec{\ell}$.
Repeating for $i=1,\,\ldots,\,s^d$, we compute all of $\vec{\ell}$.

Consider now a set of Lagrangian points $\{\vec{X}_j\}$ all in the same grid
cell. For fixed $i$, the values $\#(\lfloor\vec{X}_j\rceil+\vec{\sigma}_i)$ are
identical for all $j$, and would therefore contend with one another if
attempting to write values in parallel and independently. In this case, we
can use the well-know parallel reduce algorithm to accumulate values in
parallel and ultimately use a single thread to write the value to $\vec{\ell}$.

Generally, we do not expect to have either of these situations. In fact, it is
recommended in the IB literature that, for a connected interface, there be
1--2 ($d=2$) or 1--4 ($d=3$) Lagrangian points in each occupied grid cell. We
can combine the two ideas above using the so-called parallel segmented reduce
algorithm. Given a list of \emph{keys} and a list of \emph{values}, the
algorithm sums (or \emph{reduces}) consecutive values if their corresponding
keys match. The output is a list of non-repeating keys (though they may not be
unique within the list) and a generally shorter list of values.

If we ensure that Lagrangian points in the same grid cell have the same key
and are listed consecutively, we can use the segmented reduce algorithm to
accumulate values for support point corresponding to a fixed shift
$\vec{\sigma}$ for all Lagrangian points at once. Repeating this for each
shift, we will have completely computed $\vec{\ell}$. To achieve this ordering
of Lagrangian points, we introduce the key-value sort, which, given a list
of keys and values, will sort the values according to the keys. The output is
a sorted list of keys and a permuted list of values. If we choose the values to
be the list from 0 to $n_\gamma-1$, the resulting values define a permutation
matrix $P$. This permuted list is analogous to the linked-list structure of
[@McQueen:...], but performs better on architectures where computational units
are not independent. Unlike [@McQueen:...], we reconstruct our lists every
timestep instead of using an update. An update can be done by partitioning
Lagrangian points into those that have stayed in their grid cell and those that
have left, sorting by key the indices of just the points that have left their
cell, and then merging them back into the main list. However, the worst-case
runtime of the update is the same as simply reconstructing the list every
timestep, but has the added cost of computing a partition and merging lists.

Lastly, we need suitable way to generate keys. A function $\mathfrak{K}$ that
generates keys should be 1-to-1 with grid cells. In other words, $\mathfrak{K}$
should be bijective with grid points in $\bar{\Omega}_h$. For this reason, it
is often useful to formulate $\mathfrak{K}$ as a function of $\mathbb{Z}^d$ and
so $\mathfrak{K}(\lfloor\vec{X}\rceil)$ gives the key for a Lagrangian point
$\vec{X}$. However, bijectivity alone will, in general, invalidate $\#$ as an
otherwise good choice for $\mathfrak{K}$. However, since $\mathfrak{K}$ is
bijective, $\mathfrak{K}$ is invertible, so any Lagrangian point with key $k$
has grid index $\#(\mathfrak{K}^{-1}(k))$, if it exists. It is also desirable
that $\mathfrak{K}$ be independent of the shift $\vec{\sigma}$. If these
conditions are met and the keys have a partial order, we can compute a key for
each Lagrangian point, apply the key-value sort once, and then compute values
and use segmented reduce once per shift.

The corresponding matrices $\mathcal{S}_i$ each have at most one nonzero entry
per column. Using key-value sort to define a permutation matrix $P$ allows us
to write
\begin{equation}
    \label{eq:submatrix}
    \mathcal{S}_i\vec{L}_i = (\mathcal{S}_i'P)(P^\dagger\vec{L}),
\end{equation}
where $\vec{L}_i=P^\dagger\vec{L}$ for $i=1,\,\ldots,\,s^d$, and the matrix
$\mathcal{S}_i'$ is the same size as $\mathcal{S}$, but is constructed by
copying only the value in each column corresponding to shift $\vec{\sigma}_i$.
The matrix $\mathcal{S}_i=\mathcal{S}_i'P$ can be characterized as having
a sparse block structure where each block is a single dense row of values, each
row contains at most one block, and no two blocks have a column in common. The
new block structure allows for easier parallelization, which the segmented
reduce algorithm performs handily. Again, we have computed $\vec{\ell}$ without
keeping more than a single entry of $\mathcal{S}$ in memory per thread at a
time.

\subsection{Optimizations}
As described above, the algorithms uses $s^d$ sweeps to compute $\vec{\ell}$.
However, for each sweep, any thread will perform identically the same
operations. We can therefore consider computing multiple values of $\delta_h$
for some fixed set of shifts, and the corresponding grid indices. Since
$\mathfrak{K}$ is independent of the shift, we need compute it only once, and
we can use $P$ for any set of shifts. To avoid write contentions, we need as
many output vectors as shifts in the set. To each shift, we assign an output
vector and we write all values for that shift to the corresponding output
vector.

In other words, we can imagine computing $t$ values at once. We then expect to 
have $w=\lceil s^d/t\rceil$ sweeps and we compute
\begin{equation}
    \label{eq:fused-sweeps}
    \begin{alignedat}{5}
        \mathcal{S}\vec{L} 
        &= (\mathcal{S}_1P^\dagger\vec{L} + &\cdots& + \mathcal{S}_wP^\dagger\vec{L})
        &+& \cdots
        &+& (\mathcal{S}_{(t-1)w+1}P^\dagger\vec{L} + &\cdots& + \mathcal{S}_{s^d}P^\dagger\vec{L}) \\
        &=& \vec{\ell}_1 &&+& \cdots &+&& \vec{\ell}_t, &
    \end{alignedat}
\end{equation}
where in the first sweep, the first product in each set of parentheses is
stored in the corresponding output vector $\vec{\ell}_i$; in the second sweep,
the second product is stored in the corresponding output vector; and so on,
until all of the products have been computed. Finally, we accumulate values,
$\vec{\ell} = \vec{\ell}_1 + \cdots + \vec{\ell}_t$. The only requirement for
this optimization is that there is enough memory to hold all of the output
vectors. Choosing values of $t$ that divide $s^d$ and are around 10 seem to
give good results.

So far, we have assumed that vector-valued quantities are discretized on
staggered grids. On a uniform grid, where each component of a vector-valued
quantity is discretized at the same spatial coordinate, the grid cells are
identical for each component. In that case, each component will give the same
sort key and values will be written to output entries with the same index, so
we can compute sort keys and perform the sort once, and re-use the resulting
$P$ for each component, all of which can be computed by the same thread.
