\section{Introduction}

Many problems in biophysics involve the interaction of an incompressible fluid and an
immersed elastic interface. The solution to these problems can be approximated using the
immersed boundary (IB) method, which was developed by Charles Peskin to simulate blood
flow through the heart \cite{Peskin:1972wa}. This method couples the equations governing
fluid velocity and pressure to those governing the interface movement and elastic forces
via operations we will refer to as \term{interpolation} and \term{spreading}. Fluid
properties are discretized on an Eulerian grid with fixed Cartesian locations. The
interface is represented by a set of Lagrangian points whose Cartesian locations vary in
time. This presents a problem for effective parallelization of interpolation and
spreading, as these operators must be reconstructed at each timestep to account for the
motion of the interface. The Eulerian and Lagrangian points, on the other hand, can be
treated as fixed for several timesteps, if not for the entirety of a simulation. We may
therefore treat solving the fluid equations and computing elastic forces as an
implementation detail and focus primarily on the interpolation and spreading operations.
The strength of the IB method is its ability to be incorporated into a Navier-Stokes
solver with relative ease. This has led to its popularity in myriad applications (See,
e.g. \cite{Iaccarino:2005ii,Griffith:2020hi} and references therein).

We are interested primarily in the case of neutrally buoyant, elastic immersed structures
that move at the local fluid velocity, but do not impose it. In particular, we are
motivated by the desire to simulate whole blood, composed of red blood cells (RBCs) and
platelets immersed in blood plasma, in a vessel lined by endothelial cells. Approximately
40\% of the volume in healthy human blood is occupied by RBCs, and so even small domains
may require tens or hundreds of thousands of IB points to discretize these cells.

To take advantage of modern computing architectures, with ever-increasing numbers of
processors, it is necessary to develop parallel algorithms for the IB method.
McQueen and Peskin \cite{McQueen:1997kw} present a domain decomposition scheme to
parallelize the interpolation and spreading operations on the Cray C-90 computer with
shared memory and a modest number of vector processors. Their results illustrate the need
for a fast interpolation and spreading: even parallelized, they spend roughly half of the
wall clock time spreading and interpolating. Fai, \textit{et al.} \cite{Fai:2013do}
adapted, and successfully used, this domain decomposition scheme for use on a general
purpose graphical processing unit (GPGPU).  An alternative is to distribute work among
several devices, each with their own memory. This idea underpins the popular IBAMR
library \cite{Griffith:2007uk}, which adaptively refines the mesh around the immersed
structure. The cuIBM \cite{Layton:2011um} and PetIBM \cite{Chuang:2018ej} libraries
implement the IB method for prescribed motion on single- and multi-GPU architectures,
respectively.

GPGPUs have severe restrictions on their parallelization. Individual computational units
(``threads'' or ``work-items'') are collected in groups (``warps'' or ``wavefronts'') of
32 or 64 threads. Work is divided among coordinated thread arrays (CTAs, ``blocks'', or
``workgroups''), which comprise several warps, whose threads may or may not contribute to
the overall computation. GPUs run code using single instruction, multiple data
parallelism, which means that each active thread in a CTA executes the same instruction
on its own datum. Threads in the same warp which attempt to perform separate operations
contribute to warp divergence, which instead of performing calculations in parallel,
forces each thread to perform both operations serially, but keeps only the desired
result. GPUs are equipped with their own memory hierarchies; the relevant portions of
which are global, shared, and register memory, in increasing read/write speed. The global
memory is accessible to all threads. While reading and writing to global memory is slow,
reads and writes within a warp to sequential memory locations requires minimal
round-trips. It is therefore desirable to minimize global memory use. Shared memory is
accessible to all threads in a CTA, so a computation can be performed collaboratively
within a CTA. It is the preferred method of sharing data among threads. Register memory
is, with certain exceptions, limited to a single thread.  However, shared and register
memory are limited, and exceeding those limits typically results in fewer CTAs operating
in parallel. CPUs, on the other hand, do not have such restrictions on memory accesses,
and multicore CPUs can perform vastly different tasks in parallel without issue. The
restrictions on the GPU lead us to treat the GPU as the lowest common denominator; an
effective algorithm for the GPU should translate well to other architectures.

Here, we discuss the parallelization of interpolation and introduce a new parallelization
for the spreading operation. We demonstrate that the concurrency of these algorithms
scales with the number of IB points, independently of the Eulerian grid. The algorithms
divide these operations into trivially parallelizable tasks and parallel primitives. They
are therefore suitable for use on GPGPUs. We begin with an introduction to the IB method,
and the role of the interpolation and spreading operations.

% vim: cc=90 tw=89
