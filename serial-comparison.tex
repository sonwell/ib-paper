\subsubsection{Strong scaling}
We now consider how runtime varies for the test problem with $n=2^{16}$ IB
points, a grid refinement of 64 (grid size $h=0.25\um$), and using the OTF
variant of Algorithm \ref{algo:par-spread} with different numbers of threads.
We use up to 32 threads on the CPU and 64--4096 threads on the GPU.


%\begin{table}[h]
%\begin{center}
%\bgroup
%\renewcommand{\arraystretch}{1.7}
%\begin{tabular}{cccc}
%                                                                                  \toprule
%    $p$  & device & \titletable{interpolate}{2000} & \titletable{spread}{1000} \\ \midrule
%         & CPU    & $1.30763 \pm 0.01795$          & $1.33840 \pm 0.01221$     \\
%    1    & CPU    & $1.30437 \pm 0.00986$          & $1.51528 \pm 0.02790$     \\
%    2    & CPU    & $0.67160 \pm 0.00605$          & $0.80398 \pm 0.02135$     \\
%    4    & CPU    & $0.35833 \pm 0.00284$          & $0.44432 \pm 0.01592$     \\
%    8    & CPU    & $0.21166 \pm 0.00773$          & $0.28799 \pm 0.04476$     \\
%    16   & CPU    & $0.10044 \pm 0.00258$          & $0.15393 \pm 0.01553$     \\
%    32   & CPU    & $0.07411 \pm 0.00068$          & $0.12394 \pm 0.01759$     \\
%    64   & GPU    & $0.80977 \pm 0.00478$          & $0.96698 \pm 0.00047$     \\
%    128  & GPU    & $0.43929 \pm 0.00352$          & $0.49359 \pm 0.00064$     \\
%    256  & GPU    & $0.22434 \pm 0.00223$          & $0.26208 \pm 0.00145$     \\
%    512  & GPU    & $0.11258 \pm 0.00150$          & $0.14286 \pm 0.00177$     \\
%    1024 & GPU    & $0.05681 \pm 0.00097$          & $0.08252 \pm 0.00150$     \\
%    2048 & GPU    & $0.03074 \pm 0.00071$          & $0.05351 \pm 0.00103$     \\
%    4096 & GPU    & $0.01689 \pm 0.00047$          & $0.03905 \pm 0.00093$     \\ \bottomrule
%\end{tabular}
%\egroup
%\caption{%
%    Strong scaling results for OTF and grid spacing $h = 0.5\um$ (a grid
%    refinement of 64) for $2^{16}$ randomly placed IB points in a
%    $16\um\times16\um\times16\um$ triply periodic domain. The first row
%    are reference values for serial Algorithms \ref{algo:par-interp} and
%    \ref{algo:serial-spread}.
%}
%\end{center}
%\end{table}

%1  & CPU & $1.29442 \pm 0.00359$ & $1.50377 \pm 0.00234$
%2  & CPU & $0.65481 \pm 0.00271$ & $0.92804 \pm 0.00147$
%4  & CPU & $0.34712 \pm 0.00166$ & $0.44697 \pm 0.00148$
%8  & CPU & $0.18004 \pm 0.00084$ & $0.24580 \pm 0.00082$
%16 & CPU & $0.09969 \pm 0.00075$ & $0.14895 \pm 0.00136$
%32 & CPU & $0.07371 \pm 0.00083$ & $0.12225 \pm 0.00143$

\begin{figure}[h]
\begin{tikzpicture}
\begin{axis}[
        xmin=0.666666666666,
        xmax=6144,
        xmode=log,
        log basis x=2,
        ymode=log,
        ymin=0.5,
        ymax=128,
        log basis y=2,
        log origin=infty,
        width=\textwidth,
        height=0.5\textwidth,
        axis lines=center,
        xlabel={threads},
        ylabel={speedup},
        xlabel near ticks,
        ylabel near ticks,
        ytick={1, 4, 16, 64},
        legend style={at={(0.5, 0.9)}, anchor=center, draw=none,
                      /tikz/every even column/.append style={column sep=5pt}},
        legend cell align={left},
        legend columns=2
    ]
    \addplot+[only marks, mark=diamond*, color=tol/vibrant/magenta, mark options={fill=tol/vibrant/magenta}] coordinates {%
        (1 , {1.30763/1.29442})
        (2 , {1.30763/0.65481})
        (4 , {1.30763/0.34712})
        (8 , {1.30763/0.18004})
        (16, {1.30763/0.09969})
        (32, {1.30763/0.07371})
    };
    \addlegendentry{CPU interpolate}
    \addplot+[only marks, mark=square*, color=tol/vibrant/teal, mark options={fill=tol/vibrant/teal}] coordinates {%
        (1 , {1.33840/1.50377})
        (2 , {1.33840/0.92804})
        (4 , {1.33840/0.44697})
        (8 , {1.33840/0.24580})
        (16, {1.33840/0.14895})
        (32, {1.33840/0.12225})
    };
    \addlegendentry{CPU spread}
    \addplot+[only marks, mark=*, color=tol/vibrant/orange, mark options={fill=tol/vibrant/orange}] coordinates {%
        (64  , {1.30763/0.80977})
        (128 , {1.30763/0.43929})
        (256 , {1.30763/0.22434})
        (512 , {1.30763/0.11258})
        (1024, {1.30763/0.05681})
        (2048, {1.30763/0.03074})
        (4096, {1.30763/0.01689})
    };
    \addlegendentry{GPU interpolate}
    \addplot+[only marks, mark=triangle*, color=tol/vibrant/blue, mark options={fill=tol/vibrant/blue}] coordinates {%
        (64  , {1.33840/0.96698})
        (128 , {1.33840/0.49359})
        (256 , {1.33840/0.26208})
        (512 , {1.33840/0.14286})
        (1024, {1.33840/0.08252})
        (2048, {1.33840/0.05351})
        (4096, {1.33840/0.03905})
    };
    \addlegendentry{GPU spread}
    \addplot+[no marks, dashed, black, domain=1:32] {x^0.93};
    \addplot+[no marks, dotted, black, domain=1:32] {0.88*x^0.89};
    \addplot+[no marks, densely dashed, black, domain=64:4096] {1.61*(x/64)^0.93};
    \addplot+[no marks, densely dotted, black, domain=64:4096] {1.38*(x/64)^0.93};
\end{axis}
\end{tikzpicture}
\caption{%
    Strong scaling results for OTF and grid spacing $h = 0.5\um$ (a grid
    refinement of 64) for $2^{16}$ randomly placed IB points in a
    $16\um\times16\um\times16\um$ triply periodic domain. Speedup is measured
    relative to serial Algorithms \ref{algo:par-interp} and
    \ref{algo:serial-spread}. The four dashed or dotted trendlines give
    estimates for the percentage of the computation is done in parallel:
    approximately 93\% for CPU and GPU interpolation, and approximately 89\%
    for CPU spread. The trendline for GPU spread also indicates 93\%
    parallelization, but calls to the \texttt{thrust} library for GPU code may
    use more (or fewer) threads than listed.
}
\label{fig:unstructured-strong}
\end{figure}

Figure \ref{fig:unstructured-strong} shows the results of these tests. Speedup
is measured relative to the serial interpolation and spread implementations.
The trendlines yield estimates for the percentage of work that is done in
parallel for parallel spread and interpolate: approximately 93\% for CPU and
GPU interpolation, and approxmately 89\% for CPU spread. {\color{red} We remind
the reader that it is not trivial to limit the number of threads used by
\texttt{thrust} for work done on the GPU, so the key-value sort and segmented
reduce use as many threads as \texttt{thrust} decides is prudent. While the
trendline indicates a 93\% parallel fraction, this is merely an approximation.}

Parallel CPU interpolate using a single processor is identical to the serial
CPU interpolate, so the CPU interpolate passes through $(1,\,1)$. The same is
not true of the parallel spread using a single processor compared to serial
spread. Because of the additional sort step in the parallel spread algorithm,
single-threaded Algorithm \ref{algo:par-spread} is about 12\% slower than its
serial counterpart. The CPU code also enjoys the benefit of using vector
registers for some of the computation. {\color{red}This, coupled with the
faster processors, accounts for most of the difference in speed between the CPU
and the GPU for the same number of threads.}

\bgroup\color{red}
Interestingly, even at 4096 threads,  parallel interpolate on the GPU shows no
indication of plateauing. The final data point for that curve shows a speedup
of approximately 77$\times$. Figure \ref{fig:grid-dependence}, on the other
hand, shows that the maximum speedup we can expect for this problem is 
approximately 84$\times$, which the trendline predicts will occur at
approximately 4480 threads. Thus, we can expect the plateau for parallel
interpolate on the GPU to be very abrupt. The parallelization may be limited
by the GPUs register file size, 256KB, where each thread is using 14 32-bit
registers.
\egroup

\subsubsection{Weak scaling}
\bgroup\color{red}
WTS that for increasing work and threads proportionally, run time does not
change.
\egroup


\begin{table}
    \begin{center}
        \begingroup
        \setlength{\tabcolsep}{9pt}
        \renewcommand{\arraystretch}{1.5}
        \begin{tabular}{ccccc}
                                                                                              \toprule
            $p$  & $n$      & \titletable{interpolate}{20000} & \titletable{spread}{10000} \\ \midrule
            128  & $2^{16}$ & $0.43930 \pm 0.00019$           & $0.47632 \pm 0.00142$      \\
            256  & $2^{17}$ & $0.44918 \pm 0.00056$           & $0.46503 \pm 0.00026$      \\
            512  & $2^{18}$ & $0.45072 \pm 0.00061$           & $0.44533 \pm 0.00028$      \\
            1024 & $2^{19}$ & $0.45442 \pm 0.00049$           & $0.43561 \pm 0.00024$      \\ \bottomrule
        \end{tabular}
        \endgroup
    \end{center}
    \caption{%
        Weak scaling results for $p$ threads and $n$ randomly placed IB points
        in a $16\um\times16\um\times16\um$ triply periodic domain with
        $h=0.5\um$ on the GPU.
    }
    \label{tab:unstructured-weak}
\end{table}

Table \ref{tab:unstructured-weak} lists runtimes for increasing threads and
problem size on the GPU. Interpolate scales nearly perfectly with a difference
of 15\si{\milli\second} (\textasciitilde3\%) increase between the problem with
128 threads and $2^{16}$ IB points and that with 1024 threads and $2^{19}$
points. Spread, on the other hand, decreases in time as the problem size
increases.  \textcolor{red}{Because of the segmented reduce: density goes from
1 point per every 4 grid cells to 2 points per grid cell. Would expect this to
plateau off, and probably start increasing, for even larger problems, when the
density surpasses the tile size \texttt{thrust}'s segmented reduce uses,
whatever that may be.}
