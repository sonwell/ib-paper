\subsubsection{Strong scaling}\label{sec:unst-strong}

It is commonly the case that one wishes to employ parallelization to improve runtimes for
a problem of interest. To illustrate this improvement, we now consider how runtime varies
for the test problem with $n=2^{16}$ IB points, a grid refinement of 64 (grid size
$h=0.25\um$), and Algorithm \ref{algo:otf-spread} with different numbers of threads. We
use up to 32 threads on the CPU and 64--4096 threads on the GPU. For a fixed problem, we
ideally wish to see the runtime using $2p$ threads to be half of that using $p$ threads.
In other words, using twice as many threads yield an ideal speedup of 2.


%\begin{table}[h]
%\begin{center}
%\bgroup
%\renewcommand{\arraystretch}{1.7}
%\begin{tabular}{cccc}
%                                                                                  \toprule
%    $p$  & device & \titletable{interpolate}{2000} & \titletable{spread}{1000} \\ \midrule
%         & CPU    & $1.30763 \pm 0.01795$          & $1.33840 \pm 0.01221$     \\
%    1    & CPU    & $1.30437 \pm 0.00986$          & $1.51528 \pm 0.02790$     \\
%    2    & CPU    & $0.67160 \pm 0.00605$          & $0.80398 \pm 0.02135$     \\
%    4    & CPU    & $0.35833 \pm 0.00284$          & $0.44432 \pm 0.01592$     \\
%    8    & CPU    & $0.21166 \pm 0.00773$          & $0.28799 \pm 0.04476$     \\
%    16   & CPU    & $0.10044 \pm 0.00258$          & $0.15393 \pm 0.01553$     \\
%    32   & CPU    & $0.07411 \pm 0.00068$          & $0.12394 \pm 0.01759$     \\
%    64   & GPU    & $0.80977 \pm 0.00478$          & $0.96698 \pm 0.00047$     \\
%    128  & GPU    & $0.43929 \pm 0.00352$          & $0.49359 \pm 0.00064$     \\
%    256  & GPU    & $0.22434 \pm 0.00223$          & $0.26208 \pm 0.00145$     \\
%    512  & GPU    & $0.11258 \pm 0.00150$          & $0.14286 \pm 0.00177$     \\
%    1024 & GPU    & $0.05681 \pm 0.00097$          & $0.08252 \pm 0.00150$     \\
%    2048 & GPU    & $0.03074 \pm 0.00071$          & $0.05351 \pm 0.00103$     \\
%    4096 & GPU    & $0.01689 \pm 0.00047$          & $0.03905 \pm 0.00093$     \\ \bottomrule
%\end{tabular}
%\egroup
%\caption{%
%    Strong scaling results for OTF and grid spacing $h = 0.5\um$ (a grid
%    refinement of 64) for $2^{16}$ randomly placed IB points in a
%    $16\um\times16\um\times16\um$ triply periodic domain. The first row
%    are reference values for serial Algorithms \ref{algo:par-interp} and
%    \ref{algo:serial-spread}.
%}
%\end{center}
%\end{table}

%1  & CPU & $1.29442 \pm 0.00359$ & $1.50377 \pm 0.00234$
%2  & CPU & $0.65481 \pm 0.00271$ & $0.92804 \pm 0.00147$
%4  & CPU & $0.34712 \pm 0.00166$ & $0.44697 \pm 0.00148$
%8  & CPU & $0.18004 \pm 0.00084$ & $0.24580 \pm 0.00082$
%16 & CPU & $0.09969 \pm 0.00075$ & $0.14895 \pm 0.00136$
%32 & CPU & $0.07371 \pm 0.00083$ & $0.12225 \pm 0.00143$

\begin{figure}[htbp]
\begin{tikzpicture}
\begin{groupplot}[
    group style={group name=unst-strong, group size=1 by 2},
    width=0.45\textwidth
]
\nextgroupplot[
        xmin=0.70710678118,  % approx. 1/√2
        xmax=45.2548339959,  % approx 32√2
        xmode=log,
        log basis x=2,
        ymode=log,
        ymin=0.5,
        ymax=128,
        log basis y=2,
        log origin=infty,
        width=0.45\textwidth,
        height=0.45\textwidth,
        axis lines=center,
        ylabel={speedup},
        xlabel near ticks,
        ylabel near ticks,
        xtick={1, 2, 4, 8, 16, 32},
        ytick={1, 4, 16, 64},
        legend style={at={(0.5, 0.9)}, anchor=center, draw=none,
                      /tikz/every even column/.append style={column sep=5pt}},
        legend cell align={left},
        legend columns=1
    ]
    \addplot+[only marks, mark=diamond*, color=tol/vibrant/magenta, mark options={scale=2, fill=tol/vibrant/magenta}] coordinates {%
        (1 , {1.30763/1.29442})
        (2 , {1.30763/0.65481})
        (4 , {1.30763/0.34712})
        (8 , {1.30763/0.18004})
        (16, {1.30763/0.09969})
        (32, {1.30763/0.07371})
    }; \label{plot:unst-strong-interp}
    \addplot+[only marks, mark=triangle*, color=tol/vibrant/blue, mark options={scale=2, fill=tol/vibrant/blue}] coordinates {%
        (1 , {1.33840/1.50377})
        (2 , {1.33840/0.92804})
        (4 , {1.33840/0.44697})
        (8 , {1.33840/0.24580})
        (16, {1.33840/0.14895})
        (32, {1.33840/0.12225})
    }; \label{plot:unst-strong-spread}
    \addplot+[no marks, dashed, black, domain=1:32] {x^0.93};
    \addplot+[no marks, dotted, black, domain=1:32] {0.88*x^0.89};
    \addplot+[no marks, black, domain=1:32] {x};
    \node[fill=white] at (rel axis cs: 0.075, 0.95) {\sffamily(a)};
\nextgroupplot[
        xmin=45.2548339959,  % approx 32√2
        xmax=5792.61875148,  % approx 4096√2
        xmode=log,
        log basis x=2,
        ymode=log,
        ymin=0.5,
        ymax=128,
        log basis y=2,
        log origin=infty,
        width=0.45\textwidth,
        height=0.45\textwidth,
        axis lines=center,
        xlabel={threads},
        ylabel={speedup},
        xlabel near ticks,
        ylabel near ticks,
        ytick={1, 4, 16, 64},
        legend style={at={(0.5, 0.9)}, anchor=center, draw=none,
                      /tikz/every even column/.append style={column sep=5pt}},
        legend cell align={left},
        legend columns=1
    ]
    
    \addplot+[only marks, mark=diamond*, color=tol/vibrant/magenta, mark options={scale=2, fill=tol/vibrant/magenta}] coordinates {%
        (64  , {1.30763/0.80977})
        (128 , {1.30763/0.43929})
        (256 , {1.30763/0.22434})
        (512 , {1.30763/0.11258})
        (1024, {1.30763/0.05681})
        (2048, {1.30763/0.03074})
        (4096, {1.30763/0.01689})
    };
    \addplot+[only marks, mark=triangle*, color=tol/vibrant/blue, mark options={scale=2, fill=tol/vibrant/blue}] coordinates {%
        (64  , {1.33840/0.96698})
        (128 , {1.33840/0.49359})
        (256 , {1.33840/0.26208})
        (512 , {1.33840/0.14286})
        (1024, {1.33840/0.08252})
        (2048, {1.33840/0.05351})
        (4096, {1.33840/0.03905})
    };
    \addplot+[no marks, dashed, black, domain=64:4096] {1.61*(x/64)^0.93};
    \addplot+[no marks, dotted, black, domain=64:4096] {1.38*(x/64)^0.93};
    \addplot+[no marks, black, domain=64:4096] {1.61*x/64};
    \node[fill=white] at (rel axis cs: 0.075, 0.95) {\sffamily(b)};
\end{groupplot}
\path (unst-strong c1r2.south west|-current bounding box.south)--
coordinate(legendpos) (unst-strong c1r2.south east|-current bounding box.south);
\matrix[
    matrix of nodes,
    anchor=north,
    inner sep=0.2em,
    %draw
  ]at([yshift=-1ex]legendpos)
  {
    \ref{plot:unst-strong-interp}& Algorithm \ref{algo:par-interp}&[5pt]
    \ref{plot:unst-strong-spread}& Algorithm \ref{algo:otf-spread}\\};
\end{tikzpicture}
\caption{%
Strong scaling results for Algorithm \ref{algo:otf-spread} and grid spacing
$h = 0.5\um$ (a grid refinement of 64) for $2^{16}$ randomly placed IB points
in a $16\um\times16\um\times16\um$ triply periodic domain for (a) 1-32 CPU
cores, and (b) 64-4096 threads on the GPU. Speedup is measured relative to
serial Algorithms \ref{algo:par-interp} and \ref{algo:serial-spread}. The solid
black lines show the trendline for ideal speedup. The dashed or dotted lines
give the initial trend for interpolation and spreading, respectively.
}
\label{fig:unstructured-strong}
\end{figure}

Figure \ref{fig:unstructured-strong} shows the results of these tests. Speedup is
measured relative to the serial interpolation and spreading implementations.  The
trendlines estimate that increasing computing resources by a factor of two decreases
runtime by a factor of about 1.91 for CPU and GPU interpolation and by a factor of about
1.85 for CPU spreading. It is not trivial to limit the number of threads used by
{\thrust} for work done on the GPU, so the key-value sort and segmented reduce use as
many threads as {\thrust} decides is prudent.  While the trendline indicates a decrease
in runtime by a factor of 1.91 as well, this is merely an approximation.  

Parallel CPU interpolation using a single processor is identical to the serial
CPU interpolate, so the CPU interpolate passes through $(1,\,1)$. The same is
not true of parallel spreading using a single processor compared to serial
spreading. Because of the additional sort step in the parallel spreading
algorithm, single-threaded Algorithm \ref{algo:par-spread} is about 12\% slower
than its serial counterpart. The CPU code also enjoys the benefit of using
vector registers for some of the computation. The GPU requires 64 threads to
match the speed of a single CPU core.
%Based on the trendline in Figure
%\ref{fig:unstructured-strong} and the speedup for grid refinement shown in
%Figure \ref{fig:grid-dependence}, we see that the GPU attains a speedup of
%approximately 5.25$\times$ compared to 32 CPU processors, and uses
%approximately 4480 threads, where the kernel reaches the limit on register
%memory.
%
Even at 4096 threads, interpolate on the GPU shows no indication of plateauing. The final
data point for that curve shows a speedup of approximately $77\times$. Figure
\ref{fig:grid-dependence}(b), on the other hand, shows that the maximum speedup we can
expect for this problem is approximately 85$\times$, which the trendline in Figure
\ref{fig:unstructured-strong}(b) predicts will occur at approximately 4480 threads. Thus,
we can expect the plateau for interpolate on the GPU to be very abrupt. This indicates a
hardware limitation, the likely culprit being exhaustion of register memory. The
plateauing of the CPU curves is not a limitation of the algorithm for the CPU. Despite
having 48 cores, the test using 32 cores did not utilize them at full capacity. Using
fewer cores, on the other hand, was able to maintain full utilization for the duration of
the test. If not for having a comparatively limited number of CPU cores, we expect to see
the CPU trend continue.

If not for hardware limitations, it seems that the algorithm scales without bound on
either the CPU of GPU. Overall, trends for the CPU and GPU are very similar. Because of
these similarities, we will restrict ourselves to the GPU, but expect any conclusions to
hold for the CPU as well.

\subsubsection{Weak scaling}\label{sec:unst-weak}
In contrast, with improved computing resources, we may wish to solve bigger problems. The
ideal parallel algorithm solves a problem with $p$ threads in the same time as it solves
a twice bigger problem with $2p$ threads. Here, we place between $2^{16}$ and $2^{19}$
points randomly in the domain. We increase the number of threads proportionally, between
128 and 1024.  

\begin{table}[ht]
    \begin{center}
        \begingroup
        \setlength{\tabcolsep}{9pt}
        \renewcommand{\arraystretch}{1.5}
%        \begin{tabular}{ccccc}
%                                                                                               \toprule
%            $p$  & $n$      & \titletable{interpolate}{20000}  & \titletable{spread}{10000} \\ \midrule
%            128  & $2^{16}$ & $0.43930 \pm 0.00372 $           & $0.47632 \pm 0.00278 $     \\
%            256  & $2^{17}$ & $0.44918 \pm 0.01097 $           & $0.46503 \pm 0.00050 $     \\
%            512  & $2^{18}$ & $0.45072 \pm 0.01195 $           & $0.44533 \pm 0.00054 $     \\
%            1024 & $2^{19}$ & $0.45442 \pm 0.00960 $           & $0.43561 \pm 0.00047 $     \\ \bottomrule
%        \end{tabular}
        \begin{tabular}{ccccc}
                                                                                               \toprule
            $p$  & $n$      & \titletable{interpolate}{20000} & \titletable{spread}{10000} \\ \midrule
            128  & $2^{16}$ & $0.43930$                       & $0.47632$                  \\
            256  & $2^{17}$ & $0.44918$                       & $0.46503$                  \\
            512  & $2^{18}$ & $0.45072$                       & $0.44533$                  \\
            1024 & $2^{19}$ & $0.45442$                       & $0.43561$                  \\ \bottomrule
        \end{tabular}                                                                                             \endgroup
    \end{center}
    \caption{%
Weak scaling results for interpolation and spreading for $p$ threads and $n$ randomly
placed IB points in a $16\um\times16\um\times16\um$ triply periodic domain with
$h=0.5\um$ on the GPU. 
%95\% confidence intervals for average time per call are reported
Average time per call is reported
in seconds. $N$ is the number of samples taken.
    }
    \label{tab:unstructured-weak}
\end{table}

Table \ref{tab:unstructured-weak} lists runtimes for increasing threads and
problem size on the GPU. Interpolate scales nearly perfectly with a difference
of $15\ms$ (\textasciitilde3\%) increase between the problem with 128 threads
and $2^{16}$ IB points and that with 1024 threads and $2^{19}$ points. Spread,
on the other hand, decreases in time as the problem size increases. This
speedup is artificial, and should not be expected in general. In the $n=2^{16}$
case, there is 1 IB point for every 4 grid cells, on average. When $n=2^{19}$,
the density increases to 2 for every grid cell. As a result, it becomes
increasingly unlikely to find a cell containing no IB points. This means that
writing the values to the output vector(s) becomes increasingly coalesced,
which, in turn, reduces the number of writes to global memory and vastly
improves the speed of the write overall. Typical use of the IB method does not
have IB points in every grid cell, but the recommendations that IB points on
connected structures be spaced $0.5h$--$h$ apart typically yields 1--4 IB
points in each occupied grid cell. We now consider a more typical use of the IB
method.

% vim: cc=90 tw=89
