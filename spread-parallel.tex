\subsection{Parallel spreading}

We return now to spreading. Consider a fixed $j$ in Algorithm \ref{algo:serial-spread}
so $\shift_j$ is fixed. If every IB point inhabits its own grid cell, the support point
for each IB point corresponding to $\shift_j$ is unique. In this case, we can spread to
those support points without concern for write contentions. This is unlikely to occur in
practice. On the other hand, if every IB point were in the same grid cell, values could
be accumulated in parallel before being spread. This too is unlikely to occur in
practice. So instead, we can employ the segmented reduce algorithm, which, given a list
of values and a corresponding list of keys, will sum (reduce) consecutive values as long
as their keys match. The result is a potentially shorter list of reduced values and a
list of non-repeating keys, though they may not be unique within the list. Suppose we
were able to order the keys and values so that repeated keys are listed consecutively.
The result is a list of unique keys and all values with the same key are combined. Assign
each grid cell a unique index, and for each IB point, use the index for the grid cell it
inhabits as its key. Then, segmented reduce accumulates values spread from IB points in
the same grid cell for the given shift. Now, we have at most one value per grid cell, and
can write without contentions.

To ensure that repeated keys are listed consecutively, we use key-value sort, which,
given a list of values and a corresponding list of keys, sorts values according to a
partial ordering imposed on the keys. The result is a sorted list of keys and a permuted
list of values, but key-value pairs remain unchanged. Computing keys and values, sorting
by key, and applying the segmented reduce algorithm once per shift spreads all values.
But notice that sorting once per shift results in the same sorted list of keys each time.
Instead of sorting values, we can construct a permutation by sorting the indices of the
IB points. This need only be done once per spread operation, and using the permutation,
we can construct the list of values in sorted order for each shift. Now, we apply
segmented reduction to the sorted list of keys and newly constructed list of values.
Finally, we write the reduced values to the output. Computing values, reducing, and
writing for each shift completes the calculation.

\begin{algorithm}
\caption{Parallel spread}
\label{algo:par-spread}
\begin{algorithmic}[1]
\Procedure{parallel-spread}{$\interface_h,\,\domain_h,\,\vec{L}$}
\State $\triangleright\ $\textbf{generate}: Values of $\ell$ sampled at each point in $\domain_h$
\For {$i = 1,\,\ldots,\,n_\gamma$ \textbf{parallel}} \label{line:par-spread-ps} \Comment{Loop over IB points}
    \State $K_i \gets \key(\idx{\X_i})$ \Comment{Sort key}
    \State $P_i \gets i$ \Comment{Initial ordering}
\EndFor
\State \textbf{sort} $\{P_i\}$ \textbf{by} $\{K_i\}$ \Comment{$\{P_i\}$ now defines a permutation} \label{line:par-spread-sort}
\State $q \gets \text{\textbf{count unique} }\{K_i\}$ \label{line:par-spread-q}
\For  {$j = 1,\,\ldots,\,s^d$} \label{line:par-spread-shifts} \Comment{Loop over shifts}
    \State $\{K'_i\} \gets \{K_i\}$
    \For {$i = 1,\,\ldots,\,n_\gamma$ \textbf{parallel}} \label{line:par-spread-v} \Comment{Loop over IB points}
        \State $p \gets P_i$
        \State $\x \gets h(\idx{\X_p}+\stag)$\Comment{$\X_p\in\interface_h$, $\x\in\domain_h$}
        \State $\Delta\x \gets \x-\X_i$
        \State $w \gets \Dirac_h(\Delta\x+h\shift_j)$
        \State $V_i \gets w \cdot L_p$
    \EndFor \label{line:par-spread-vend}
    \textbf{reduce} $\{V_i\}$ \textbf{by} $\{K'_i\}$ \label{line:par-spread-reduce} \Comment{Segmented reduce}
    \For {$i = 1,\,\ldots,\,q$ \textbf{parallel}} \label{line:par-spread-quse}\Comment{Loop over inhabited grid cells, $q \le n_\gamma$}
        \State $\x \gets h(\key^{-1}(K'_i) + \stag)$
        \State $m \gets \#(\idx{\x} + \shift_j)$ \Comment{$\idx{\x}\equiv\key^{-1}(K'_i)$}
        \If {$m\neq\error$}
            \State $\ell_m \gets \ell_m + V_i$
        \EndIf
    \EndFor \label{line:par-spread-wend}
\EndFor
\State \Return $\vec{\ell}$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\input{algo-figure}

Lastly, we need suitable way to generate keys. A function $\key$ that generates keys
should be 1-to-1 with grid cells, or alternatively, to points in $\bar{\domain}_h$. For
this reason, it is useful to formulate $\key$ as a function of $\mathbb{Z}^d$ and so
$\key(\idx{\X})$ gives the key for IB point $\X$. The requirement that $\key$ be
injective will, in general, invalidate the grid indexing function as an otherwise good
choice, as it maps each ghost point in $\bar{\domain}_h$ to $\error$. We can, however,
use the key to compute grid indices: the point in $\domain_h$ with key $k$ also has grid
index $\#(\key^{-1}(k))$. Because $\key$ is injective, $\key^{-1}(k)$ is well-defined,
and for shift $\shift$, $\#(\key^{-1}(k)+\shift)$ yields the appropriate target index for
writing to the output vector. Putting these pieces together, we have a complete parallel
spreading algorithm, listed in Algorithm \ref{algo:par-spread} and illustrated in Figure
\ref{fig:algo}.

Lines \ref{line:par-spread-ps}--\ref{line:par-spread-sort} of Algorithm
\ref{algo:par-spread} construct the permutation by constructing a list of keys and a list
of the indices of IB points, $1,\,2,\,\ldots,\,n_\gamma$, and sorting the indices
according to the keys. The $i\th$ entry in the permuted list of indices gives the index
of the $i\th$ IB point in sorted order. On line \ref{line:par-spread-q}, we define $q$
to be the number of unique keys, which is also the number of reduced values to write.
Since the list of sorted keys does not change for different shifts, we compute it once
and reuse the value; see lines \ref{line:par-spread-quse}--\ref{line:par-spread-wend}.
Values, one per IB point, are computed and stored in a list $\vec{V}$ on lines
\ref{line:par-spread-v}--\ref{line:par-spread-vend}. These values are then input to the
segmented reduce, line \ref{line:par-spread-reduce}.

Algorithm \ref{algo:par-spread} does not explicitly depend on the Eulerian grid, with
some caveats dependent upon implementation details. Our implementation relies on the
{\thrust} library to provide the key-value sort, segmented reduce, and unique counting
routines. Sorting is implemented as radix sort, which has a runtime of
$\bigo{wn_\gamma/p}$, where $p$ is the number of processors/threads, and $w$ is the
number of bits required to represent every key. In general, $w\propto\log_2 n$, for $n$
elements to be sorted, but we use 32-bit integers for keys, so $w=32$. It is reasonable
to assume that this is true for most use-cases, as there are approximately $n_\omega$
possible keys, and implementations of \texttt{BLAS} and \texttt{LAPACK} typically use
32-bit integers for indexing. However, extremely fine grids with more than $2^{32}$ grid
points will require a larger data type to represent each key uniquely. In that case, $w$
increases with a finer grid. Segmented reduction has a much more complicated relationship
with the Eulerian grid. Parallelized segmented reduction has a worst-case runtime of
$\bigo{n_\gamma/p}$, but the constant of proportionality depends on the density of IB
points within inhabited grid cells. On the one hand, if all IB points inhabit the same
grid cell, segmented reduce proceeds as a regular reduce, which is very fast. On the
other hand, if every IB point inhabits its own grid cell, there is no work to be done,
and any time spent by the algorithm is for naught. The density of IB points also affects
the value of $q$, but is bounded above by $n_\gamma$. For up to $2^{32}$ grid points, we
expect the overall runtime of the spreading algorithm to be $\bigo{n_\gamma/p}$.

Finally, we remark that Algorithm \ref{algo:par-spread} must synchronize threads once per
shift. For choices of $\kernel$ where $s^d$ is large, this can hurt performance. This
requirement is written as a parallel loop (line \ref{line:par-spread-v}) within a serial
loop (line \ref{line:par-spread-shifts}). To reduce the number of synchronizations, we
must be able to compute and store several values at once. However, attempting to write
multiple values of once may lead to contentions. Below, we develop algorithms that
require fewer synchronizations by using more memory to spread several values at once and
avoid write contentions.

\subsection{Buffered spreading variants}

Here we introduce a buffer in which to store incomplete calculations. It can be thought
of as a set of temporary output vectors, which we will combine at the end of the
algorithm to finish the calculation. These vectors have $n_\omega$ entries, so adding
them requires as many operations, but is quite easily parallelized, and is provided by
any parallel \texttt{BLAS} implementation. In this regard, these buffered variants depend
explicitly on the Eulerian grid. The only additional requirement is that there is enough
memory to hold the buffer. 

To develop these variants, we note that the sorted list of keys and the permutation
are the same for all shifts. Since the permutation dictates the order of the values to
spread, and the list of keys decides the behavior of the segmented reduce, these steps
are nearly identical for each shift, save for the effect each shift might have on the
values. We can therefore use several shifts to compute one value of $\Dirac_h$ each, and
use the same machinery to accumulate the values as if they were vectors. Then, to avoid
write contentions, each of the entries of the reduced vectors is written to a separate
output vector of the buffer. We choose $\bufsz\approx 10$ values to compute each
iteration. Summing the vectors in the buffer yields the desired result.

\begin{algorithm}
\caption{Buffered parallel spreading (pre-allocated buffer)}
\label{algo:pa-spread}
\begin{algorithmic}[1]
\Procedure{pa-buffer-parallel-spread}{$\interface_h,\,\domain_h,\,\vec{L}, \vec{\ell}_1,\,\ldots,\,\vec{\ell}_\bufsz$}
\State $\triangleright\ $\textbf{require}: $\bufsz \ge 1$
\State $\triangleright\ $\textbf{generate}: Values of $\ell$ sampled at each point in $\domain_h$
\For {$i = 1,\,\ldots,\,n_\gamma$ \textbf{parallel}} \Comment{Loop over IB points}
    \State $K_i \gets \key(\idx{\X_i})$ \Comment{Sort key}
    \State $P_i \gets i$ \Comment{Initial ordering}
\EndFor
\State \textbf{sort} $\{P_i\}$ \textbf{by} $\{K_i\}$
\State $q \gets \text{\textbf{count unique} }\{K_i\}$
\For {$j = 1,\,\ldots,\,\ceil{s^d/\bufsz}$} \Comment{Loop over shifts}
    \State $\{K'_i\} \gets \{K_i\}$
    \For {$i = 1,\,\ldots,\,n_\gamma$ \textbf{parallel}} \Comment{Loop over IB points}
        \State $p \gets P_i$
        \State $\x \gets h(\idx{\X}+\stag)$\Comment{$\X_p\in\interface_h$, $\x\in\domain_h$}
        \State $\Delta\x \gets \x-\X_i$
        \For {$k=1,\,\ldots,\,\textbf{min}(\bufsz,\,s^d-\bufsz\cdot j)$}
            \State $w \gets \Dirac_h(\Delta\x+h\shift_{\bufsz\cdot j + k})$
            \State $V_{ik} \gets w \cdot L(\X_p)$ \Comment{$V\in\mathbb{R}^{n_\gamma\times\bufsz}$}
        \EndFor
    \EndFor
    \State \textbf{reduce} $\{V_{i\cdot}\}$ \textbf{by} $\{K'_i\}$
    \For {$i = 1,\,\ldots,\,q$ \textbf{parallel}} \Comment{Loop over inhabited grid cells, $q\le n_\gamma$}
        \State $\x \gets h(\key^{-1}(K'_i) + \stag)$
        \For {$k=1,\,\ldots,\,\textbf{min}(\bufsz,\,s^d-\bufsz\cdot j)$}
            \State $m \gets \#(\x + \shift_{\bufsz\cdot j + k})$
            \If {$m\neq\error$}
                \State $\ell_{mk} \gets \ell_{mk} + V_{ik}$
            \EndIf
        \EndFor
    \EndFor
\EndFor
\State \Return $\vec{\ell}_1 + \cdots + \vec{\ell}_\bufsz$
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Buffered parallel spreading (on-the-fly buffer allocation)}
\label{algo:otf-spread}
\begin{algorithmic}[1]
\Procedure{otf-buffer-parallel-spread}{$\interface_h,\,\domain_h,\,\vec{L}$}
\State $\triangleright\ $\textbf{require}: $\bufsz \ge 1$
\State $\triangleright\ $\textbf{generate}: Values of $\ell$ sampled at each point in $\interface_h$
\For {$i = k,\,\ldots,\,\bufsz$}
    \State $\vec{\ell}_k \gets \vec{0}$
\EndFor
\State \Return \Call{pa-buffer-parallel-spread}{$\interface_h,\,\domain_h,\,\vec{L},\,\vec{\ell}_1,\,\ldots,\,\vec{\ell}_\bufsz$} \Comment{Algorithm \ref{algo:pa-spread}}
\EndProcedure \Comment{Lifetime of $\vec{\ell}_k$ ends here}
\end{algorithmic}
\end{algorithm}

Algorithm \ref{algo:pa-spread} lists the general form of the algorithm as pseudocode, and
Algorithm \ref{algo:otf-spread} gives a minor modification. The difference between these
two variants is the lifetime of the buffer: Algorithm \ref{algo:pa-spread} does not
manage the buffer itself, but Algorithm \ref{algo:otf-spread} allocates and frees the
buffer memory, limiting the lifetime of the buffer to the duration of the algorithm. The
latter considers the memory allocation as part of the algorithm. The additional overhead
from buffer allocation means that we never expect Algorithm \ref{algo:otf-spread} to
outperform Algorithm \ref{algo:pa-spread}; it is provided as an alternative for systems
where maintaining a large, long-lived buffer may exhaust memory. We compare these
algorithms and Algorithm \ref{algo:par-spread} to Algorithm \ref{algo:serial-spread}, and
the efficacy of Algorithm \ref{algo:par-interp} below.

% vim: cc=90 tw=89
